{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Discrete state (observation space)\n",
    "\n",
    "1. MDP (VI, PI, dynamic programming)\n",
    "2. Q-learning\n",
    "3. Deep Q network (DQN)\n",
    "   \n",
    "   (1) Deep Q learning with Atariï¼Ÿ(mainly based on discrete action-value functins, that is Q-function. In such a case, the continutity of the state space may cause the dimension of the action space to be too large, making the storage and calulation of the Q table or Q network impratical???)\n",
    "\n",
    "   (2) Double DQN: use two independent Q networkd to estimate the Q table, one of which selects the acition and the other evaluates the value of action. This helps to slow down the overestimation bias of Q-value estimation and improve the stability of learning.\n",
    "   \n",
    "* Continuous state\n",
    "  \n",
    "1. Distrbutional Q network: hard ro implement\n",
    "\n",
    "2. Policy gradient\n",
    "   \n",
    "   (1) Deep deterministic policy gradient (DDPG) (combnine **deterministic policy gradient** and **deep Q learning**; achieved good results in dealing with environments that require **continuous action**, such as robotic arm control, mobile tobots, etc.)\n",
    "\n",
    "3. Advantage Actor-Critic (A2C) (combine the learning of value functions and the learning of strategies)\n",
    "   \n",
    "4. Deep Actor-Critic (A3C) (introduce asychronous training to interact and learn from each other through multiple agents running in parrel to improve training efficiency)\n",
    "   \n",
    "5. Proximal policy optimization (PPO) (based on policy gradient)\n",
    "\n",
    "2-5: These methods is to directly learn the strategy (probablity distribution of actions) rather than the state-value function (Q-function).\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
